============================= test session starts ==============================
platform linux -- Python 3.12.7, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/capybara/code/ml/capybaradb
configfile: pytest.ini
collected 76 items

tests/test_integration.py ..............                                 [ 18%]
tests/test_main.py ..................                                    [ 42%]
tests/test_model.py ...FFFF.....F                                        [ 59%]
tests/test_utils.py ........................F......                      [100%]

=================================== FAILURES ===================================
_________________ TestEmbeddingModel.test_embed_single_string __________________

self = <tests.test_model.TestEmbeddingModel object at 0x70406377ee40>
mock_model = <MagicMock name='AutoModel' id='123421849347488'>
mock_tokenizer = <MagicMock name='AutoTokenizer' id='123421849349408'>
sample_text = 'This is a sample text for testing purposes.'

    @patch("capybaradb.model.AutoTokenizer")
    @patch("capybaradb.model.AutoModel")
    def test_embed_single_string(self, mock_model, mock_tokenizer, sample_text):
        mock_tokenizer_instance = MagicMock()
        mock_tokenizer.from_pretrained.return_value = mock_tokenizer_instance
    
        mock_model_instance = MagicMock()
        mock_model.from_pretrained.return_value = mock_model_instance
    
        mock_tokenizer_instance.return_value = {
            "input_ids": torch.tensor([[1, 2, 3]]),
            "attention_mask": torch.tensor([[1, 1, 1]]),
        }
    
        mock_model_instance.return_value = [torch.randn(1, 3, 384)]
    
        model = EmbeddingModel()
>       result = model.embed(sample_text)
                 ^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_model.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
capybaradb/model.py:43: in embed
    sentence_embeddings = self._mean_pooling(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <capybaradb.model.EmbeddingModel object at 0x7040637f6c60>
model_output = <MagicMock name='AutoModel.from_pretrained().to()()' id='123421848644944'>
attention_mask = tensor([[1, 1, 1]])

    def _mean_pooling(self, model_output, attention_mask):
        token_embeddings = model_output[0]
        input_mask_expanded = (
>           attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        )
E       RuntimeError: expand(torch.LongTensor{[1, 3, 1]}, size=[1]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3)

capybaradb/model.py:76: RuntimeError
________________ TestEmbeddingModel.test_embed_list_of_strings _________________

self = <tests.test_model.TestEmbeddingModel object at 0x70406377efc0>
mock_model = <MagicMock name='AutoModel' id='123421847734848'>
mock_tokenizer = <MagicMock name='AutoTokenizer' id='123421847737152'>
sample_documents = ['Machine learning is a subset of artificial intelligence.', 'Deep learning uses neural networks with multiple layers.', 'Natural language processing deals with text and speech.']

    @patch("capybaradb.model.AutoTokenizer")
    @patch("capybaradb.model.AutoModel")
    def test_embed_list_of_strings(self, mock_model, mock_tokenizer, sample_documents):
        mock_tokenizer_instance = MagicMock()
        mock_tokenizer.from_pretrained.return_value = mock_tokenizer_instance
    
        mock_model_instance = MagicMock()
        mock_model.from_pretrained.return_value = mock_model_instance
    
        mock_tokenizer_instance.return_value = {
            "input_ids": torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),
            "attention_mask": torch.tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]]),
        }
    
        mock_model_instance.return_value = [torch.randn(3, 3, 384)]
    
        model = EmbeddingModel()
>       result = model.embed(sample_documents)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_model.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
capybaradb/model.py:43: in embed
    sentence_embeddings = self._mean_pooling(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <capybaradb.model.EmbeddingModel object at 0x7040636c1ee0>
model_output = <MagicMock name='AutoModel.from_pretrained().to()()' id='123421848255664'>
attention_mask = tensor([[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]])

    def _mean_pooling(self, model_output, attention_mask):
        token_embeddings = model_output[0]
        input_mask_expanded = (
>           attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        )
E       RuntimeError: expand(torch.LongTensor{[3, 3, 1]}, size=[1]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3)

capybaradb/model.py:76: RuntimeError
________________ TestEmbeddingModel.test_embed_binary_precision ________________

self = <tests.test_model.TestEmbeddingModel object at 0x70406377f140>
mock_model = <MagicMock name='AutoModel' id='123421847839488'>
mock_tokenizer = <MagicMock name='AutoTokenizer' id='123421847841600'>
sample_text = 'This is a sample text for testing purposes.'

    @patch("capybaradb.model.AutoTokenizer")
    @patch("capybaradb.model.AutoModel")
    def test_embed_binary_precision(self, mock_model, mock_tokenizer, sample_text):
        mock_tokenizer_instance = MagicMock()
        mock_tokenizer.from_pretrained.return_value = mock_tokenizer_instance
    
        mock_model_instance = MagicMock()
        mock_model.from_pretrained.return_value = mock_model_instance
    
        mock_tokenizer_instance.return_value = {
            "input_ids": torch.tensor([[1, 2, 3]]),
            "attention_mask": torch.tensor([[1, 1, 1]]),
        }
    
        mock_model_instance.return_value = [torch.randn(1, 3, 384)]
    
        model = EmbeddingModel(precision="binary")
>       result = model.embed(sample_text)
                 ^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_model.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
capybaradb/model.py:43: in embed
    sentence_embeddings = self._mean_pooling(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <capybaradb.model.EmbeddingModel object at 0x7040636d7860>
model_output = <MagicMock name='AutoModel.from_pretrained().to()()' id='123421848377328'>
attention_mask = tensor([[1, 1, 1]])

    def _mean_pooling(self, model_output, attention_mask):
        token_embeddings = model_output[0]
        input_mask_expanded = (
>           attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        )
E       RuntimeError: expand(torch.LongTensor{[1, 3, 1]}, size=[1]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3)

capybaradb/model.py:76: RuntimeError
_______________ TestEmbeddingModel.test_embed_float16_precision ________________

self = <tests.test_model.TestEmbeddingModel object at 0x70406377f110>
mock_model = <MagicMock name='AutoModel' id='123421847834880'>
mock_tokenizer = <MagicMock name='AutoTokenizer' id='123421847843424'>
sample_text = 'This is a sample text for testing purposes.'

    @patch("capybaradb.model.AutoTokenizer")
    @patch("capybaradb.model.AutoModel")
    def test_embed_float16_precision(self, mock_model, mock_tokenizer, sample_text):
        mock_tokenizer_instance = MagicMock()
        mock_tokenizer.from_pretrained.return_value = mock_tokenizer_instance
    
        mock_model_instance = MagicMock()
        mock_model.from_pretrained.return_value = mock_model_instance
    
        mock_tokenizer_instance.return_value = {
            "input_ids": torch.tensor([[1, 2, 3]]),
            "attention_mask": torch.tensor([[1, 1, 1]]),
        }
    
        mock_model_instance.return_value = [torch.randn(1, 3, 384)]
    
        model = EmbeddingModel(precision="float16")
>       result = model.embed(sample_text)
                 ^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_model.py:135: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
capybaradb/model.py:43: in embed
    sentence_embeddings = self._mean_pooling(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <capybaradb.model.EmbeddingModel object at 0x704063a34260>
model_output = <MagicMock name='AutoModel.from_pretrained().to()()' id='123421847739792'>
attention_mask = tensor([[1, 1, 1]])

    def _mean_pooling(self, model_output, attention_mask):
        token_embeddings = model_output[0]
        input_mask_expanded = (
>           attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        )
E       RuntimeError: expand(torch.LongTensor{[1, 3, 1]}, size=[1]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3)

capybaradb/model.py:76: RuntimeError
___________________ TestEmbeddingModel.test_device_handling ____________________

self = <tests.test_model.TestEmbeddingModel object at 0x70406377f350>
mock_model = <MagicMock name='AutoModel' id='123421847718080'>
mock_tokenizer = <MagicMock name='AutoTokenizer' id='123421847722496'>

    @patch("capybaradb.model.AutoTokenizer")
    @patch("capybaradb.model.AutoModel")
    def test_device_handling(self, mock_model, mock_tokenizer):
        mock_tokenizer_instance = MagicMock()
        mock_tokenizer.from_pretrained.return_value = mock_tokenizer_instance
    
        mock_model_instance = MagicMock()
        mock_model.from_pretrained.return_value = mock_model_instance
    
        model = EmbeddingModel(device="cuda")
    
        mock_tokenizer_instance.return_value = {
            "input_ids": torch.tensor([[1, 2, 3]]),
            "attention_mask": torch.tensor([[1, 1, 1]]),
        }
    
        mock_model_instance.return_value = [torch.randn(1, 3, 384)]
    
        with patch("torch.cuda.is_available", return_value=True):
>           result = model.embed("test")
                     ^^^^^^^^^^^^^^^^^^^

tests/test_model.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
capybaradb/model.py:43: in embed
    sentence_embeddings = self._mean_pooling(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <capybaradb.model.EmbeddingModel object at 0x704063642840>
model_output = <MagicMock name='AutoModel.from_pretrained().to()()' id='123421847853616'>
attention_mask = tensor([[1, 1, 1]], device='cuda:0')

    def _mean_pooling(self, model_output, attention_mask):
        token_embeddings = model_output[0]
        input_mask_expanded = (
>           attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        )
E       RuntimeError: expand(torch.cuda.LongTensor{[1, 3, 1]}, size=[1]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3)

capybaradb/model.py:76: RuntimeError
_____________ TestOCRProcessor.test_extract_text_from_image_error ______________

self = <tests.test_utils.TestOCRProcessor object at 0x704063796db0>
mock_vision_model = <MagicMock name='VisionEncoderDecoderModel' id='123421848246784'>
mock_processor = <MagicMock name='TrOCRProcessor' id='123421846714416'>
mock_cuda = <MagicMock name='is_available' id='123421846669120'>

    @patch("capybaradb.utils.torch.cuda.is_available", return_value=False)
    @patch("capybaradb.utils.TrOCRProcessor")
    @patch("capybaradb.utils.VisionEncoderDecoderModel")
    def test_extract_text_from_image_error(
        self, mock_vision_model, mock_processor, mock_cuda
    ):
        mock_processor_instance = MagicMock()
        mock_processor.from_pretrained.return_value = mock_processor_instance
        mock_processor_instance.return_value.side_effect = Exception("Test error")
    
        mock_vision_model_instance = MagicMock()
        mock_vision_model.from_pretrained.return_value = mock_vision_model_instance
    
        processor = OCRProcessor()
    
        mock_image = Mock(spec=Image.Image)
    
        result = processor.extract_text_from_image(mock_image)
>       assert result == ""
E       AssertionError: assert <MagicMock name='TrOCRProcessor.from_pretrained().batch_decode().__getitem__().strip()' id='123421847742048'> == ''

tests/test_utils.py:302: AssertionError
=============================== warnings summary ===============================
.venv/lib/python3.12/site-packages/PyPDF2/__init__.py:21
  /home/capybara/code/ml/capybaradb/.venv/lib/python3.12/site-packages/PyPDF2/__init__.py:21: DeprecationWarning: PyPDF2 is deprecated. Please move to the pypdf library instead.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/test_model.py::TestEmbeddingModel::test_embed_single_string - Ru...
FAILED tests/test_model.py::TestEmbeddingModel::test_embed_list_of_strings - ...
FAILED tests/test_model.py::TestEmbeddingModel::test_embed_binary_precision
FAILED tests/test_model.py::TestEmbeddingModel::test_embed_float16_precision
FAILED tests/test_model.py::TestEmbeddingModel::test_device_handling - Runtim...
FAILED tests/test_utils.py::TestOCRProcessor::test_extract_text_from_image_error
=================== 6 failed, 70 passed, 1 warning in 3.17s ====================
